
\begin{document}

\chapter{Current Approach}

\label{chapter:current_appraoch}

This chapter identifies the main source of comparison in literature, then it proceeds to describe the tracking algorithm.

The current work is an extension of Wei et al.\'s study~\cite{wei_kinect_calibration}. It applies the same algorithms for doing calibration and coordinate transformation on the skeletal joints. The system will use the transformation technique to convert a skeleton's spatial position in one Kinect to another Kinect's field of view. Subsequently, the system can merge skeletons of the same person from different Kinects fields of view to achieve persistent tracking. The technology allows the system to continuously track people when the targets are obstructed by obstacles in the scene, as long as they remain in the extended field of view. 

Both work uses only two Kinects, but the current work does a wider range of experiments. Wei et al.\ places the Kinects side by side. The current system is evaluated with Kinects that are not only adjacent to each other but further away from each other and have larger angle gaps. The former study also uses only one participant, whereas the current work uses up to two people in evaluation and can theoretically track up to six people, where the number of tracked person is limited by the maximum number of skeletons a single Kinect can recognize. Wei et al.\ uses simple movements to measure the performance of the tracking system. On the other hand, the current work introduces more complex movements. Occlusion is not discussed in Wei et al.\'s study.

\section{Running the application}
\label{sec:current_approach_running_application}

KinectMultiTrack is available at \url{https://github.com/cjw-charleswu/KinectMultiTrack}. The application should run on the Microsoft Windows operating system. It is developed, tested, and maintained on Windows 8.

The application consists of a server and client. There must be at least one server and one client when running the system. It is recommended that there be at least two client. One machine can run both as the server and client.

Running the server:

\textbf{Type ``\textbackslash{KinectMultiTrack}\textbackslash{KinectMultiTrack}\textbackslash{MultiTrackServer}\textbackslash{bin}\textbackslash{Release}\textbackslash{KinectMultiTrack}.exe [port]'' into the Windows Command Prompt or Windows PowerShell.}

The port number is optional. If the field is empty, the server will start listening for connections at port 12345.

Running the client:

\textbf{Type ``\textbackslash{KinectMultiTrack}\textbackslash{KinectMultiTrack}\textbackslash{MultiTrackClient}\textbackslash{bin}\textbackslash{Release}\textbackslash{MultiTrackClient}.exe [address] [port]''  into the Windows Command Prompt or Windows PowerShell.}

The address and port number are option. If the fields are empty, the client will send BodyFrames to the researcher's machine.

\section{Overview}
\label{sec:current_approach_overview}

The complete system architecture will be explained in Section~\ref{sec:design_architecture}. In short, the clients send Kinect BodyFrames to the server. Later, the server process the data, performs calibration and tracking. Wei et al.\ designed a system where each individual machine running a Kinect will perform its own calibration then transmit the calibrated, transformed coordinates to the server~\cite{wei_kinect_calibration}. The current approach puts the burden of processing on the server machine, and it requires less computational resources on the client side. Essentially, the clients are like Kinect hotspots sending skeletal information to the server. The tradeoffs between the two different approaches are not investigated.

Calibration allows the current system to precisely convert the skeletal joints coordinates from the original kinect's camera space into the new coordinate system, also known as the World coordinate system. After calibration, the system will combine multiple skeletons of each tracked person from different Kinects fields of view. This constitutes the initial tracking result. The system matches the skeletons based on their proximity of spatial positions in the World View, or the field of view of the World coordinate system.

When the server receives a new BodyFrame from the client, which is effectively a process running a Kinect sensor, it will update the spatial positions of the skeletons in the particular field of view. The person whose skeletons comprised of that skeleton will now have an updated position in the World View. The server handles multiple clients, hence multiple Kinects, in parallel to perform tracking.

The tracking process relies heavily on the initial result created after calibration. Any imperfect transformation process will produce errors, so as the current system. In this current context, error refers to the coordinates distances between multiple skeletons when converged from different Kinects fields of view into the same field of view. For instance, the system will try its best at merging skeletons of the same person from different Kinects fields of view into the same positions in the World View. Since the transformation process is not perfect, the underlying algorithm will produce two skeletons in the World View that have similar, but not exactly the same, positions. The system will minimize the skeletal joint differences by constructing an average skeleton.

The following chapter will explain all the steps leading to the transformed skeletons in a single viewing perspective.

\section{Serialization}
\label{sec:current_approach_serialization}

The Kinect BodyFrame is the wrapper around skeleton information at each frame. The same data structure was called SkeletonFrame in the Kinect v1 SDK. The BodyFrames are assembled by the Kinect sensor internally from its depth data; they provide a high level API for programming with skeletons. Each BodyFrame contains at most six Kinect Bodies, where each Body represents a person's skeleton from the Kinect's field of view. A Body encloses the skeletal joints coordinates and other related metadata. The current system uses this preprocessed information to track individuals in the scene.

The current Kinect v2 SDK does not support running multiple Kinects on a single machine. Therefore, the researcher writes a simple TCP server and client framework to pass Kinect BodyFrames from the clients to the server. TCP sockets deal with data in bytes, therefore the server and clients must exchange serialized data. Unfortunately, the Kinect v2 SDK does not support serialization of the Kinect BodyFrame. To resolve such inconvenience, the researcher also develops a Kinect BodyFrame serialization library. See Appendix~\ref{sec:appendix_bodyframe_serialization} for a complete list of serialized data. The most important pieces of information are the skeletal joints and their tracking states, because the system requires targets' spatial positions as well as Kinect's confidence level about their joint positions. The system will give more weights to actively tracked skeletal joints when creating the average skeleton.

\section{Calibration}
\label{sec:current_approach_calibration}

The calibration procedure requires each skeleton's initial center position and angle between itself and the Kinect. The number of skeletons to be calibrated increases with the number of Kinects. For example, if there is only one person in the scene with two Kinects, there is a total of two skeletons, one from each of the Kinect's field of view, and so on.

Wei et al.\ defined the initial center position and angle as follows~\cite{wei_kinect_calibration}:

\begin{description}
  \item[Initial center position] A skeleton's initial center position is its average of all joints coordinates over the duration of calibration. The center position is represented as $C(x_c, y_c, z_c)$, where $x_c$, $y_c$, and $z_c$ are the $x$, $y$, and $z$ coordinates, respectively.
  \item[Initial angle] A skeleton's initial angle is the angle between the skeleton and Kinect in the last frame. The initial angle is represented as $\theta$.
\end{description}

The derivation of the initial center position shown in Equation~\ref{eq:initial_center_position}. $J$ denotes the number of joints per skeleton. $N$ equals $25$ in the current Kinect SDK. $T$ denotes the number of frames used for calibration. $T$ equals $120$ in both the Wei et al.\ and the current study. $S(x_s, y_s, z_s)$ denotes the the sum of the joints coordinates in all calibration frames. $A(x_a, y_a, z_a)$ denotes the average of the joints coordinates in all calibration frames. $C(x_c, y_c, z_c)$ can be derived from the above information.

\begin{equation}[!h]
\label{eq:initial_center_position}
\begin{gathered}
S(x_s, y_s, z_s) = \sum_{t=1}^T (\sum_{j=1}^J (x_{t,j}, y_{t,j}, z_{t,j})) \\
A(x_a, y_a, z_a) = A(x_a, y_a, z_a) / N \\
C(x_c, y_c, z_c) = C(x_c, y_c, z_c) / T
\end{gathered}
\end{equation}

The derivation of the initial angle is shown in Equation~\ref{eq:initial_angle}. $Z_r$ is the right shoulder z coordinate. $Z_l$ is the left shoulder z coordinate. $X_r$ is the right shoulder x coordinate. $X_r$ is the left shoulder x coordinate.

\begin{equation}[!h]
\label{eq:initial_angle}
\begin{gathered}
D = Z_r - Z_l \\
W = X_r - X_l \\
\theta = \arctan(D/W)
\end{gathered}
\end{equation}


The system uses 120 Kinect BodyFrames for calibration, implying that the process will take at least four seconds, becasue the Kinect provides BodyFrames at 30 frames per second, and the system does not assume all Kinects start at the same time. The system will initiate the calibration process once it has received sufficient frames from all connected clients. If more frames were available from a connected Kinect, the system would use the latest 120 frames. The calibration procedure uses the coordinates in the Kinect Camera Space for all calculations.

\subsection{Coordinate transformation}

After the system completes calibration, it can transform any calibrated skeleton into the World coordinate system. Skeletal joints in the new coordinate system can also be transformed back to any Kinect's field of view. The transformation process is the very first step of filling the missing joints of a skeleton during occlusion. The average skeleton represents a person' complete joint coordinates, joined from multiple Kinects fields of view. Unsurprisingly, the accuracy of the system depends on how well the transformation algorithm is implemented.

Given the initial center position $C(X_C, Y_C, Z_C)$, initial angle $\theta$, and the number of joints per skeleton, a skeletal joint in the World coordinate system can be expressed as follows:

\begin{equation}[!h]
\label{eq:joint_translated}
J_t (X_{Jt}, Y_{Jt}, Z_{Jt}) = (X_j - X_C, Y_j - Y_C, Z_j - Z_C)
\end{equation}

\begin{equation}[!h]
\label{eq:joint_worldview}
J_w (X_{Jw}, Y_{Jw}, Z_{Jw}) = (X_{Jt}\cos\theta + Z_{Jt}\sin\theta, Y_{Jt}, Z_{Jt}\cos\theta - X_{Jt}\sin\theta)
\end{equation}

% \begin{equation}
% \label{eq:joint_backtokinect}
% \end{gathered}

% \begin{bmatrix} X_{Jt} & Z_{Jt} \end{bmatrix} = 
% \left[\begin{array}{c} X_{Jw} & Y_{Jw} \end{array} \right] \times
% \left[\begin{array}{cc} z & w \end{array} \right]

% \end{gathered}
% \end{equation}


\subsection{Detecting interference}

The system will automatically restart the calibration process if people move their joints over ten centimeters during calibration. The vaue of ten centimeters is chosen through trial and error by the researcher, below which the Kinect sensor may cause the tracking algorithm to recalibrate unecessarily due to sensor noise. The current implementation checks movements at the person's head, left hand, and right hand (S)

\begin{algorithm}[!h,width=0.9\columnwidth]
    \caption{$\Function{IsStationary}(jt, c, p, msg)$}
    \algorithmicrequire{
        \\$jt$: Joint type
        \\$c$: Serialized body in the current frame
        \\$p$: Serialized body in the previous frame
        \\$msg$: Error message
    }
    \\\algorithmicensure{
        \\Whether the person has moved the joint during calibration
    }
    \label{algo:algo_defect}
    \begin{algorithmic}[1]
        \setcounter{ALC@line}{0}
        \vspace*{1mm}

        \IF{$(c = \Null) \vee (\neg \Function{Contains}(c, jt)) \vee (\neg \Function{Contains}(p, jt))$}
          \STATE $msg \leftarrow \texttt{"Missing"} + jt$
          \RETURN $\False$
        \ENDIF

        \STATE $c\_jt \leftarrow Joint(c, jt)$
        \STATE $p\_jt \leftarrow Joint(p, jt)$
        \STATE $d \leftarrow Distance(c\_jt, p\_jt)$

        \IF{$\mbox{d} > 0.1$}
          \STATE $msg \leftarrow \texttt{""} + jt + \texttt{" remain stationary"}$
          \RETURN $\False$
        \ENDIF

        \RETURN $\True$
    \end{algorithmic}
\end{algorithm}

\section{Tracking by detection}
\label{sec:current_approach_tracking_by_detection}

After calibration, all the system sees is a collection of skeletons with their initial position and angle. As aforementioned, the system can represent these skeletons in both the Kinect Camera Space and the world coordinate system. This information is not useful on its own, and the more people there are in the views of the Kinects, the larger this collection of skeletons would be. The tracking system should know which skeletons belong to which person, thus knowing about people's absolute position relative to any Kinect field of view.

The system construct models of tracked people by finding skeletons in different fields of view that have high proximity in the world coordinate system. The system performs the detection algorithm using joints in the world coordinate system, because using these coordinates would allow the system to compare skeletons' spatial positions regardless of perspectives. It assumes that skeletons from different Kinects field of view that have the highest proximity must belong to the same person. The system continues this process until it can no longer put skeletons in pair. The pseudocode is shown below \textbf{(todo: insert pseudocode)}.

The current implementation assumes that every person is visible to all Kinects. The system works fine when there is only one person who is occluded from all Kinects, because the person would only have one skeleton, leaving it to the last to be matched. The system would not detect people correctly in scenarios where several people are occluded from all Kinects in the calibration phase, because it would try to match skeletons from different Kinect fields of view even though they are far apart from each other. \textbf{(todo: add illustrations of a working scenario and a non-working scenario)}.

Every calibration process follows a people detection result. It entails models of currently tracked people, where each model consists a number of potential skeletons, all from different Kinect perspectives. A potential skeleton represents one replica of a person from a Kinect field of view. The average skeleton of a person is the average body across all potential skeletons in the person's model. The average skeleton of a person can be seen as the system's view of that person. The result is permanent until the system recalibrates. That is, the same skeletons are always associated with the same person whom they were initially identified with. 

The system performs tracking by updating every potential skeleton in the current result, propagating the changes to the skeleton visualization.

\subsection{Detecting occlusion}

When a person obstructs another person causing a Kinect sensor to partially or completely lose the sight of the masked person, the system would fill in the joints from other actively tracked potential skeletons. The average skeleton would be calculated using only the actively tracked joints from all potential skeletons. The effect would be visible on the application front-end; the visualization would display the latest average skeleton.

\subsection{Detecting new and missing people}

The system makes the assumption that people during calibration will be the only people in the scene throughout the lifetime of the system. This is because the system does not have any information about the new people entering the scene that would otherwise be obtained during calibration, such as their precise initial position and angle, which are needed to perform coordinate transformation. When the system detects intruders or zero people in the scene, it would automatically initiate calibration. \textbf{(todo: finish coding)} Scenarios where a number of skeletons, but not all, is missing from the system's available Kinects are unimportant, because the people possessing those skeletons may only be temporarily occluded.

Since the positions of all potential skeletons are updated every frame, the system would know when the skeletons are missing. The scene is empty if and only if every potential skeleton in the scene is empty.

\end{document}
