
\begin{document}

\chapter{Current Approach}

\label{chapter:current_appraoch}

This chapter identifies the main source of comparison in literature, then it proceeds to describe the tracking algorithm.

The current work is an extension of Wei et al.\'s study~\cite{wei_kinect_calibration}. It applies the same algorithms for doing calibration and coordinate transformation on the skeletal joints. The system will use the transformation technique to convert a skeleton's spatial position in one Kinect to another Kinect's field of view. Subsequently, the system can merge skeletons of the same person from different Kinects fields of view to achieve persistent tracking. The technology allows the system to continuously track people when the targets are obstructed by obstacles in the scene, as long as they remain in the extended field of view. 

Both work uses only two Kinects, but the current work does a wider range of experiments. Wei et al.\ places the Kinects side by side. The current system is evaluated with Kinects that are not only adjacent to each other but further away from each other and have larger angle gaps. The former study also uses only one participant, whereas the current work uses up to two people in evaluation and can theoretically track up to six people, where the number of tracked person is limited by the maximum number of skeletons a single Kinect can recognize. Wei et al.\ uses simple movements to measure the performance of the tracking system. On the other hand, the current work introduces more complex movements. Occlusion is not discussed in Wei et al.\'s study.

\section{Running the application}
\label{sec:current_approach_running_application}

KinectMultiTrack is available at \url{https://github.com/cjw-charleswu/KinectMultiTrack}. The application should run on the Microsoft Windows operating system. It is developed, tested, and maintained on Windows 8.

The application consists of a server and client. There must be at least one server and one client when running the system. It is recommended that there be at least two client. One machine can run both as the server and client.

Running the server:

\textbf{Type} ``KinectMultiTrack\textbackslash{KinectMultiTrack}\textbackslash{MultiTrackServer}\textbackslash{bin}\textbackslash{Release}\textbackslash{KinectMultiTrack}.exe [port]'' \textbf{into the Windows Command Prompt or Windows PowerShell.}

The port number is optional. If the field is empty, the server will start listening for connections at port 12345.

Running the client:

\textbf{Type} ``KinectMultiTrack\textbackslash{KinectMultiTrack}\textbackslash{MultiTrackClient}\textbackslash{bin}\textbackslash{Release}\textbackslash{MultiTrackClient}.exe [address] [port]'' \textbf{into the Windows Command Prompt or Windows PowerShell.}

The server address and port number are optional. If the fields are empty, the client will send BodyFrames to the researcher's machine.

\section{Overview}
\label{sec:current_approach_overview}

The complete system architecture will be explained in Section~\ref{sec:design_architecture}. In short, the clients send Kinect BodyFrames to the server. Later, the server process the data, performs calibration and tracking. Wei et al.\ designed a system where each individual machine running a Kinect will perform its own calibration then transmit the calibrated, transformed coordinates to the server~\cite{wei_kinect_calibration}. The current approach puts the burden of processing on the server machine, and it requires less computational resources on the client side. Essentially, the clients are like Kinect hotspots sending skeletal information to the server. The tradeoffs between the two different approaches are not investigated.

Calibration allows the current system to precisely convert the skeletal joints coordinates from the original kinect's camera space into the new coordinate system, also known as the World coordinate system. After calibration, the system will combine multiple skeletons of each tracked person from different Kinects fields of view. This constitutes the initial tracking result. The system matches the skeletons based on their proximity of spatial positions in the World View, or the field of view of the World coordinate system.

When the server receives a new BodyFrame from the client, which is effectively a process running a Kinect sensor, it will update the spatial positions of the skeletons in the particular field of view. The person whose skeletons comprised of that skeleton will now have an updated position in the World View. The server handles multiple clients, hence multiple Kinects, in parallel to perform tracking.

The tracking process relies heavily on the initial result created after calibration. Any imperfect transformation process will produce errors, so as the current system. In this current context, error refers to the coordinates distances between multiple skeletons when converged from different Kinects fields of view into the same field of view. For instance, the system will try its best at merging skeletons of the same person from different Kinects fields of view into the same positions in the World View. Since the transformation process is not perfect, the underlying algorithm will produce two skeletons in the World View that have similar, but not exactly the same, positions. The system will minimize the skeletal joint differences by constructing an average skeleton.

The following chapter will explain all the steps leading to the transformed skeletons in a single viewing perspective.

\section{Serialization}
\label{sec:current_approach_serialization}

The Kinect BodyFrame is the wrapper around skeleton information at each frame. The same data structure was called SkeletonFrame in the Kinect v1 SDK. The BodyFrames are assembled by the Kinect sensor internally from its depth data; they provide a high level API for programming with skeletons. Each BodyFrame contains at most six Kinect Bodies, where each Body represents a person's skeleton from the Kinect's field of view. A Body encloses the skeletal joints coordinates and other related metadata. The current system uses this preprocessed information to track individuals in the scene.

The current Kinect v2 SDK does not support running multiple Kinects on a single machine. Therefore, the researcher writes a simple TCP server and client framework to pass Kinect BodyFrames from the clients to the server. TCP sockets deal with data in bytes, therefore the server and clients must exchange serialized data. Unfortunately, the Kinect v2 SDK does not support serialization of the Kinect BodyFrame. To resolve such inconvenience, the researcher also develops a Kinect BodyFrame serialization library. See Appendix~\ref{sec:appendix_bodyframe_serialization} for a complete list of serialized data. The most important pieces of information are the skeletal joints and their tracking states, because the system requires targets' spatial positions as well as Kinect's confidence level about their joint positions. The system will give more weights to actively tracked skeletal joints when creating the average skeleton.

\section{Calibration}
\label{sec:current_approach_calibration}

The calibration procedure requires each skeleton's initial center position and angle between itself and the Kinect. The number of skeletons to be calibrated increases with the number of Kinects. For example, if there is only one person in the scene with two Kinects, there is a total of two skeletons, one from each of the Kinect's field of view, and so on.

Wei et al.\ defined the initial center position and angle as follows~\cite{wei_kinect_calibration}:

\begin{description}
  \item[Initial center position] A skeleton's initial center position is its average of all joints coordinates over the duration of calibration. The center position is represented as $C(X_c, Y_c, Z_c)$, where $X_c$, $Y_c$, and $Z_c$ are the $x$, $y$, and $z$ coordinates, respectively.
  \item[Initial angle] A skeleton's initial angle is the angle between the skeleton and Kinect in the last frame. The initial angle is represented as $\theta$.
\end{description}

The derivation of the initial center position shown in Equation~\ref{eq:initial_center_position}. $J$ denotes the number of joints per skeleton. $N$ equals $25$ in the current Kinect SDK. $T$ denotes the number of frames used for calibration. $T$ equals $120$ in both the Wei et al.\ and the current study. $S(X_s, Y_s, Z_s)$ denotes the the sum of the joints coordinates in all calibration frames. $A(X_a, Y_a, Z_a)$ denotes the average of the joints coordinates in all calibration frames. $C(X_c, Y_c, Z_c)$ can be derived from the above information.

\begin{equation}
\label{eq:initial_center_position}
\begin{gathered}
S(X_s, Y_s, Z_s) = \sum_{t=1}^T (\sum_{j=1}^J (X_{t,j}, Y_{t,j}, Z_{t,j})) \\
A(X_a, Y_a, Z_a) = A(X_a, Y_a, Z_a) / N \\
C(X_c, Y_c, Z_c) = C(X_c, Y_c, Z_c) / T
\end{gathered}
\end{equation}

The derivation of the initial angle is shown in Equation~\ref{eq:initial_angle}. $Z_r$ is the right shoulder z coordinate. $Z_l$ is the left shoulder z coordinate. $X_r$ is the right shoulder x coordinate. $X_r$ is the left shoulder x coordinate.

\begin{equation}
\label{eq:initial_angle}
\begin{gathered}
D = Z_r - Z_l \\
W = X_r - X_l \\
\theta = \arctan(D/W)
\end{gathered}
\end{equation}

The system uses 120 Kinect BodyFrames for calibration, implying that the process will take at least four seconds, because the Kinect provides BodyFrames at 30 frames per second, and the system does not assume all Kinects start at the same time. The system will automatically start the calibration process once it has received sufficient frames from all connected Kinects. If more frames were available from a Kinect, the system will use the most recent frames for calibration. The calibration procedure uses the coordinates in the Kinect Camera Space for all calculations.

\subsection{Coordinate transformation}

Coordinate transformation is the very step in filling the missing joints of a skeleton during occlusion. After the system completes calibration, it can convert the coordinates of a calibrated skeleton into the World coordinate system. The skeletal joints in the new coordinate system can also be transformed back to any Kinect's field of view. The average skeleton represents a person whose joints are averaged over all actively tracked joints from the available fields of view. The efficacy of the tracking algorithm is determined by how close the average skeleton is away from the original skeletons from multiple views. Hence, the accuracy of the tracking system rests on the implementation of the coordinate transformation technique.

Equation~\ref{eq:joint_worldview} shows the reduction of joint in a Kinect field of view to a joint in the World view. The equation uses the initial center position $C(X_c, Y_c, Z_c)$ and angle $\theta$ from Equation~\ref{eq:initial_center_position} and Equation~\ref{eq:initial_angle} using the joint's initial values in the resulting Kinect field of view. For instance, to transform joint A from the Kinect field of view $F_1$ to another Kinect field of view $F_2$, the system would use joint A's initial center position and angle in the view $F_2$. The joint in the current Kinect field of view to be transformed is represented by $K_j(X_{k,j}, Y_{k,j}, Z_{k,j})$, where $j$ is the joint type. $W_j(X_{w,j}, Y_{w,j}, Z_{w,j})$ is the final joint coordinate in the World view, calculated using unit quaternion demonstrated by Wei et al.~\cite{wei_kinect_calibration}. 

\begin{equation}
\label{eq:joint_worldview}
\begin{gathered}
K'_j(X_{k',j}, Y_{k',j}, Z_{k',j}) = (X_{k,j}-X_c, Y_{k,j}-Y_c, Z_{k,j}-Z_c) \\
W_j(X_{w,j}, Y_{w,j}, Z_{w,j}) = (X_{k',j}\cos\theta+Z_{k',j}\sin\theta, Y_{k',j}, Z_{k',j}\cos\theta-X_{k',j}\sin\theta)
\end{gathered}
\end{equation}

A joint in the World view can be transformed back to the coordinate in a selected Kinect field of view using a simple matrix operation (See Equation~\ref{eq:joint_kinect_1}, Equation~\ref{eq:joint_kinect_2}, Equation~\ref{eq:joint_kinect_3}, and Equation~\ref{eq:joint_kinect_4}). It is worth noting that the initial center position and angle are from the same joint in final Kinect field of view.

\begin{equation}
\label{eq:joint_kinect_1}
\begin{gathered}
  A =
  \begin{pmatrix}
  \cos\theta & 0 & \sin\theta \\
  0 & 1 & 0 \\
  -\sin\theta & 0 &  \cos\theta
  \end{pmatrix}, 
  A *
  \begin{pmatrix}
  X_{k',j} \\
  Y_{k',j} \\
  Z_{k',j}
  \end{pmatrix} =
  \begin{pmatrix}
  X_{w,j} \\
  Y_{w,j} \\
  Z_{w,j}
  \end{pmatrix}
\end{gathered}
\end{equation}

\begin{equation}
\label{eq:joint_kinect_2}
\begin{gathered}
  \inv{A} =
  \begin{pmatrix}
  A_1 & A_2 & A_3 \\
  A_4 & A_5 & A_6 \\
  A_7 & A_8 & A_9
  \end{pmatrix}, 
  \begin{pmatrix}
  X_{k',j} \\
  Y_{k',j} \\
  Z_{k',j}
  \end{pmatrix} =
  \inv{A} *
  \begin{pmatrix}
  X_{w,j} \\
  Y_{w,j} \\
  Z_{w,j}
  \end{pmatrix}
\end{gathered}
\end{equation}

\begin{equation}
\label{eq:joint_kinect_3}
\begin{gathered}
  \begin{pmatrix}
  X_{k',j} \\
  Y_{k',j} \\
  Z_{k',j}
  \end{pmatrix} =
  \begin{pmatrix}
  A_1*X_{W,j}+A_3*Z_{W,j} \\
  Y_{W,j} \\
  A_7*X_{W,j}+A_9*Z_{W,j}
  \end{pmatrix}
\end{gathered}
\end{equation}

\begin{equation}
\label{eq:joint_kinect_4}
\begin{gathered}
 \begin{pmatrix}
  X_{k,j} \\
  Y_{k,j} \\
  Z_{k,j}
 \end{pmatrix} = 
 \begin{pmatrix}
  X_{k',j} + X_c\\
  Y_{w,j} + Y_c\\
  Z_{k',j} + Z_c
 \end{pmatrix}
\end{gathered}
\end{equation}

\subsection{Detecting interference}

The system will automatically restart the calibration process if people move their joints over ten centimeters during calibration. The value of ten centimeters is chosen through trial and error by the researcher, below which the Kinect sensor may cause the tracking algorithm to calibrate unnecessarily due to sensor noise. The current implementation checks movements at the person's head, left hand, and right hand (See Algorithm~\ref{algo:algo_interference}).

\begin{algorithm}[!h,width=0.9\columnwidth]
    \caption{$\Function{IsStationary}(jt, c, p, msg)$}
    \algorithmicrequire{
        \\$jt$: Joint type
        \\$c$: Serialized body in the current frame
        \\$p$: Serialized body in the previous frame
        \\$msg$: Error message
    }
    \\\algorithmicensure{
        \\Whether the person has moved the joint during calibration
    }
    \label{algo:algo_interference}
    \begin{algorithmic}[1]
        \setcounter{ALC@line}{0}
        \vspace*{1mm}

        \IF{$(c = \Null) \vee (\neg \Function{Contains}(c, jt)) \vee (\neg \Function{Contains}(p, jt))$}
          \STATE $msg \leftarrow \texttt{"Missing"} + jt$
          \RETURN $\False$
        \ENDIF

        \STATE $c\_jt \leftarrow Joint(c, jt)$
        \STATE $p\_jt \leftarrow Joint(p, jt)$
        \STATE $d \leftarrow Distance(c\_jt, p\_jt)$

        \IF{$\mbox{d} > 0.1$}
          \STATE $msg \leftarrow \texttt{""} + jt + \texttt{" remain stationary"}$
          \RETURN $\False$
        \ENDIF

        \RETURN $\True$
    \end{algorithmic}
\end{algorithm}

\section{Tracking by detection}
\label{sec:current_approach_tracking_by_detection}

After calibration, the system knows a collection of skeletons, as well with their initial position and angle relative to each Kinect. As aforementioned, the system can represent these skeletons in both the Kinect Camera Space and the World coordinate system. The individual skeletons are not useful, and the size of the collection will increase with increasing number of people in the scene. The tracking system should know which skeletons belongs to which person, thus knowing their absolute possible relative any Kinect's field of view.

The system matches skeletons of the same person in different fields of view by group them based on their proximity in the World coordinate system. The system does the matching with joint information in the World coordinate system, because coordinates in this system do not relate to any sensor, which enables the system to compare the spatial positions of the skeletons regardless of their origin. The system assumes that skeletons that are closest together in the World coordinate system must belong to the same person. The grouping process continues until it can no longer pair skeletons in the World coordinate system. At this stage, the system outputs the initial detection result based on spatial information alone.

The current implementation assumes that every person is visible to all Kinects during calibration. The system works fine even if there is one person in one camera occlusion, because system will leave it to the last to be matched, making it a single skeleton person. The system will not detect people correctly in scenarios where several people are occluded during the calibration phase, because it will continuously match skeletons as long as this more than one skeleton from multiple fields of view. For instance, the system will falsely match skeletons of two people who occluded
by the sensor in different fields of view.

The initial detection result consists of a collection of currently tracked people, where each person contains a number of potential skeletons from different Kinect fields of view. A potential skeleton a replica of a person from a single Kinect's field of view. The average skeleton of a person is the average of all potential skeletons representing the person. The average skeleton can be treated as the system's view of that person. The same skeletons are always associated with the same person whom they were initially identified with. The result is permanent until the system recalibrates. When a person is obstructed by an obstacle or by another person, the system will still try to update their joint coordinates if they are still in the viewing range of other cameras.

The system performs tracking by updating the potential skeletons with joint information from all of the Kinects. When a person obstructs another person, causing a Kinect sensor to partially or completely lose the sight of the masked person, the system use the joints coordinates from other running Kinects that have the view of the person in occlusion, if possible. The average skeleton is always calculated using only actively tracked joints.

\subsection{Detecting new and missing people}

The system makes another assumption that the people during calibration are the only people throughout the lifetime of the system. This limitation is introduced as an side effect of using coordinate transformation for calibration. Without knowing the initial center position and angle of new people, the current system cannot perform coordinate transformation their spatial position. When the system detects intruders and an empty scene, it will automatically start calibration from the beginning. The system knows the scene is empty when all the potential skeletons in the system lose spatial information in the current frame. However, the current system can cope with temporary occlusion caused by the Kinects using the ``filling the gaps'' strategy described.

\end{document}
