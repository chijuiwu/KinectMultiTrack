
\begin{document}

\chapter{Current Approach}

\label{chapter:current_appraoch}

This chapter identifies the main source of comparison in literature, then it proceeds to describe the tracking algorithm.

The current work is an extension of Wei et al.\'s study~\cite{wei_kinect_calibration}. It applies the same algorithms for doing calibration and coordinate transformation on the skeletal joints. The system will use the transformation technique to convert a skeleton's spatial position in one Kinect to another Kinect's field of view. Subsequently, the system can merge skeletons of the same person from different Kinects fields of view to achieve persistent tracking. The technology allows the system to continuously track people when the targets are obstructed by obstacles in the scene, as long as they remain in the extended field of view. 

Both work uses only two Kinects, but the current work does a wider range of experiments. Wei et al.\ places the Kinects side by side. The current system is evaluated with Kinects that are not only adjacent to each other but further away from each other and have larger angle gaps. The former study also uses only one participant, whereas the current work uses up to two people in evaluation and can theoretically track up to six people, where the number of tracked person is limited by the maximum number of skeletons a single Kinect can recognize. Wei et al.\ uses simple movements to measure the performance of the tracking system. On the other hand, the current work introduces more complex movements. Occlusion is not discussed in Wei et al.\'s study.

\section{Running the application}
\label{sec:current_approach_running_application}

KinectMultiTrack is available at \url{https://github.com/cjw-charleswu/KinectMultiTrack}. The application should run on the Microsoft Windows operating system. It is developed, tested, and maintained on Windows 8.

The application consists of a server and client. There must be at least one server and one client when running the system. It is recommended that there be at least two client. One machine can run both as the server and client.

Run the server:

{\centering
  contents\par
}

Run the client:



\section{Overview}
\label{sec:current_approach_overview}

The complete system architecture will be explained in section~\ref{sec:design_architecture}. In short, the clients send Kinect BodyFrames to the server. Later, the server process the data, performs calibration and tracking. Wei et al.\ designed a system where each individual machine running a Kinect will perform its own calibration then transmit the calibrated, transformed coordinates to the server~\cite{wei_kinect_calibration}. The current approach puts the burden of processing on the server machine, and it requires less computational resources on the client side. Essentially, the clients are like Kinect hotspots sending skeletal information to the server. The tradeoffs between the two different approaches are not investigated.

Calibration allows the current system to precisely convert the skeletal joints coordinates from the original kinect's camera space into the new coordinate system, also known as the World coordinate system. After calibration, the system will combine multiple skeletons of each tracked person from different Kinects fields of view. This constitutes the initial tracking result. The system matches the skeletons based on their proximity of spatial positions in the World View, or the field of view of the World coordinate system.

When the server receives a new BodyFrame from the client, which is effectively a process running a Kinect sensor, it will update the spatial positions of the skeletons in the particular field of view. The person whose skeletons comprised of that skeleton will now have an updated position in the World View. The server handles multiple clients, hence multiple Kinects, in parallel to perform tracking.

The tracking process relies heavily on the initial result created after calibration. Any imperfect transformation process will produce errors, so as the current system. In this current context, error refers to the coordinates distances between multiple skeletons when converged from different Kinects fields of view into the same field of view. For instance, the system will try its best at merging skeletons of the same person from different Kinects fields of view into the same positions in the World View. Since the transformation process is not perfect, the underlying algorithm will produce two skeletons in the World View that have similar, but not exactly the same, positions. The system will minimize the skeletal joint differences by constructing an average skeleton for every person.

The following chapter will explain all the steps leading to transformed skeletons in a single viewing perspective.

\section{Serialization}
\label{sec:current_approach_serialization}

The Kinect BodyFrame contains real-time skeletal information of people who are tracked by a Kinect sensor. It was called SkeletonFrame in the Kinect v1 SDK. The BodyFrames are computed by the Kinect sensor internally from its depth data; they provide a higher level API for programming with skeletons. The current system uses the preprocessed information to track individuals.

Each frame contains at most six Kinect Body data, where each Body represents a skeleton from the Kinect's field of view. A Body contains a skeleton's joints coordinates and metadata.

TCP network clients and server exchange data in bytes, thus requiring any content passed between the two nodes to be serialized. Unfortunately, the Kinect v2 SDK does not support serialization of Kinect BodyFrame. To resolve the inconvenience, the current work builds a Kinect BodyFrame serialization library. The most important pieces of information are the skeleton joints, their coordinates in the Kinect Camera Space, types and tracking states. See Appendix~\ref{sec:appendix_bodyframe_serialization} for a complete list of serialized properties.

\section{Calibration}
\label{sec:current_approach_calibration}

To perform coordinate transformation on a skeleton from its current Kinect field of view into the World coordinate system, the system requires its initial center position and initial angle between itself and the Kinect.  

The following definitions are from the Wei et al study \cite{wei_kinect_calibration}.

Initial center position is defined as the average of a skeleton's joints coordinates over the duration of calibration.

Let N = number of joints per skeleton (20). Let T = number of frames used for calibration (120).

Let S = joints sum. Let A = joints average. Let C = joints center (over time). 

\begin{equation}
\label{eq:joints_sum}
S(X_S, Y_S, Z_S) = \sum_{j=1}^J (X_j, Y_j, Z_j)
\end{equation}

\begin{equation}
\label{eq:joints_average}
A(X_A, Y_A, Z_A) = S(X_S, Y_S, Z_S) / N
\end{equation}

\begin{equation}
\label{eq:joints_center}
C(X_C, Y_C, Z_C) = A(X_A, Y_A, Z_A) / T
\end{equation}

The initial angle is defined as the the angle between the Kinect and the skeleton.

\begin{equation}
\label{eq:theta_d}
\begin{gathered}
\text{Let } Z_r = \text{Right shoulder z coordinate} \\
\text{Let } Z_l = \text{Left shoulder z coordinate} \\
D = Z_r - Z_l
\end{gathered}
\end{equation}

\begin{equation}
\label{eq:theta_w}
\begin{gathered}
\text{Let } X_r = \text{Right shoulder x coordinate} \\
\text{Let } X_l = \text{Left shoulder x coordinate} \\
W = X_r - X_l
\end{gathered}
\end{equation}

\begin{equation}
\label{eq:theta}
\theta = \arctan(D/W)
\end{equation}

The system uses 120 Kinect BodyFrames for calibration. The Kinect provides BodyFrame at 30 frames per second, meaning the calibration process will take at least four seconds. The system will initiate the calibration process once it has received sufficient frames from all connected clients. If more frames were available from a connected Kinect, the system would use the latest 120 frames. The calibration procedure uses the coordinates in the Kinect Camera Space for all calculations.

\subsection{Detecting interference}

The system will automatically restart the calibration process if people move their joints over ten centimeters during calibration. The current implementation checks movements at the person's head, left hand, and right hand.


\begin{algorithm}[ht,width=0.9\columnwidth]
    \caption{$\Function{RemainStationary}(jt, c, p, msg)$}
    \algorithmicrequire{
        \\$jt$: Joint type
        \\$c$: Serialized body in the current frame
        \\$p$: Serialized body in the previous frame
        \\$msg$: Error message
    }
    \\\algorithmicensure{
        \\Whether the person has moved the joint during calibration
    }
    \label{algo:prm}
    \begin{algorithmic}[1]
        \setcounter{ALC@line}{0}
        \vspace*{1mm}

        \IF{$(c = \Null) \vee (\neg \Function{Contains}(c, jt)) \vee (\neg \Function{Contains}(p, jt))$}
          \STATE $msg \leftarrow \texttt{"Missing"} + jt$
          \RETURN $\False$
        \ENDIF

        \STATE $c\_jt \leftarrow Joint(c, jt)$
        \STATE $p\_jt \leftarrow Joint(p, jt)$
        \STATE $d \leftarrow Distance(c\_jt, p\_jt)$

        \IF{$\mbox{d} > 0.1$}
          \STATE $msg \leftarrow \texttt{""} + jt + \texttt{" remain stationary"}$
          \RETURN $\False$
        \ENDIF

        \RETURN $\True$
    \end{algorithmic}
\end{algorithm}


\subsection{Coordinate transformation}

After the system completes calibration, it can transform any calibrated skeleton into the World coordinate system. Skeletal joints in the new coordinate system can also be transformed back to any Kinect's field of view. The transformation process is the very first step of filling the missing joints of a skeleton during occlusion. The average skeleton represents a person' complete joint coordinates, joined from multiple Kinects fields of view. Unsurprisingly, the accuracy of the system depends on how well the transformation algorithm is implemented.

Given the initial center position $C(X_C, Y_C, Z_C)$, initial angle $\theta$, and the number of joints per skeleton, a skeletal joint in the World coordinate system can be expressed as follows:

\begin{equation}
\label{eq:joint_translated}
J_t (X_{Jt}, Y_{Jt}, Z_{Jt}) = (X_j - X_C, Y_j - Y_C, Z_j - Z_C)
\end{equation}

\begin{equation}
\label{eq:joint_worldview}
J_w (X_{Jw}, Y_{Jw}, Z_{Jw}) = (X_{Jt}\cos\theta + Z_{Jt}\sin\theta, Y_{Jt}, Z_{Jt}\cos\theta - X_{Jt}\sin\theta)
\end{equation}

% \begin{equation}
% \label{eq:joint_backtokinect}
% \end{gathered}

% \begin{bmatrix} X_{Jt} & Z_{Jt} \end{bmatrix} = 
% \left[\begin{array}{c} X_{Jw} & Y_{Jw} \end{array} \right] \times
% \left[\begin{array}{cc} z & w \end{array} \right]

% \end{gathered}
% \end{equation}

\section{Tracking by detection}
\label{sec:current_approach_tracking_by_detection}

After calibration, all the system sees is a collection of skeletons with their initial position and angle. As aforementioned, the system can represent these skeletons in both the Kinect Camera Space and the world coordinate system. This information is not useful on its own, and the more people there are in the views of the Kinects, the larger this collection of skeletons would be. The tracking system should know which skeletons belong to which person, thus knowing about people's absolute position relative to any Kinect field of view.

The system construct models of tracked people by finding skeletons in different fields of view that have high proximity in the world coordinate system. The system performs the detection algorithm using joints in the world coordinate system, because using these coordinates would allow the system to compare skeletons' spatial positions regardless of perspectives. It assumes that skeletons from different Kinects field of view that have the highest proximity must belong to the same person. The system continues this process until it can no longer put skeletons in pair. The pseudocode is shown below \textbf{(todo: insert pseudocode)}.

The current implementation assumes that every person is visible to all Kinects. The system works fine when there is only one person who is occluded from all Kinects, because the person would only have one skeleton, leaving it to the last to be matched. The system would not detect people correctly in scenarios where several people are occluded from all Kinects in the calibration phase, because it would try to match skeletons from different Kinect fields of view even though they are far apart from each other. \textbf{(todo: add illustrations of a working scenario and a non-working scenario)}.

Every calibration process follows a people detection result. It entails models of currently tracked people, where each model consists a number of potential skeletons, all from different Kinect perspectives. A potential skeleton represents one replica of a person from a Kinect field of view. The average skeleton of a person is the average body across all potential skeletons in the person's model. The average skeleton of a person can be seen as the system's view of that person. The result is permanent until the system recalibrates. That is, the same skeletons are always associated with the same person whom they were initially identified with. 

The system performs tracking by updating every potential skeleton in the current result, propagating the changes to the skeleton visualization.

\subsection{Detecting occlusion}

When a person obstructs another person causing a Kinect sensor to partially or completely lose the sight of the masked person, the system would fill in the joints from other actively tracked potential skeletons. The average skeleton would be calculated using only the actively tracked joints from all potential skeletons. The effect would be visible on the application front-end; the visualization would display the latest average skeleton.

\subsection{Detecting new and missing people}

The system makes the assumption that people during calibration will be the only people in the scene throughout the lifetime of the system. This is because the system does not have any information about the new people entering the scene that would otherwise be obtained during calibration, such as their precise initial position and angle, which are needed to perform coordinate transformation. When the system detects intruders or zero people in the scene, it would automatically initiate calibration. \textbf{(todo: finish coding)} Scenarios where a number of skeletons, but not all, is missing from the system's available Kinects are unimportant, because the people possessing those skeletons may only be temporarily occluded.

Since the positions of all potential skeletons are updated every frame, the system would know when the skeletons are missing. The scene is empty if and only if every potential skeleton in the scene is empty.

\end{document}
